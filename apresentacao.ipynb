{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a499e10f",
   "metadata": {},
   "source": [
    "\n",
    "* [1.0 - Quem é a openAI?](#1_0)\n",
    "    * [1.1 - Sobre a empresa](#1_1)\n",
    "    * [1.2 - Relações com Elon Musk e Microsoft](#1_2)\n",
    "    * [1.3 - Produtos](#1_3)\n",
    "        * [1.3.1 - DALL·E](#1_3_1)\n",
    "        * [1.3.2 - Whisper](#1_3_2)\n",
    "        * [1.3.3 - Moderation](#1_3_3)\n",
    "        * [1.3.4 - Modelos de chat complition](#1_3_4)\n",
    "        * [1.3.5 - Embeddings e um pouco de como o chatGPT funciona](#1_3_5)\n",
    "* [2.0 - Customização dos modelos](#2_0)\n",
    "    * [2.1 - Prompt design/engineering](#2_1)\n",
    "    * [2.2 - Fine Tuning](#2_2)\n",
    "        * [2.2.1 - Quando usar? ](#2_2_1)\n",
    "        * [2.2.2 - Consigo fazer o fine tuning do chatCPT 3.5 e 4? ](#2_2_2)\n",
    "        * [2.2.3 - Vantages](#2_2_3)\n",
    "        * [2.2.4 - Como fazer o fine tuning?](#2_2_4)\n",
    "        * [2.2.5 - Os dados de treino serão usados para alimentar o chatGPT e se tornarem públicos?](#2_2_5)\n",
    "* [3.0 - IA pode desenvolver conciência, desejos e experiências subjetivas? ](#3_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392eca69",
   "metadata": {},
   "source": [
    "# 1.0 - Quem é a openAI?  <a class=\"anchor\" id=\"1_0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfbedf",
   "metadata": {},
   "source": [
    "## 1.1 - Sobre a empresa <a class=\"anchor\" id=\"1_1\"></a>\n",
    "\n",
    "A empresa se identifica como uma organização de pesquisa em inteligência artificial e embora a OpenAI tenha lançado alguns produtos a empresa está mais focada em pesquisa e desenvolvimento de tecnologias de IA em geral.\n",
    "\n",
    "Fundadores são empresários e pesquisadores de tecnologia, incluindo [Elon Musk](https://pt.wikipedia.org/wiki/Elon_Musk) (empresário), [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) (empresário), [Greg Brockman](https://gregbrockman.com/) (pesquisador), [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever)(pesquisador) e outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3efc1f",
   "metadata": {},
   "source": [
    "## 1.2 - Relações com Elon Musk e Microsoft <a class=\"anchor\" id=\"1_2\"></a>\n",
    "\n",
    "Segundo esta fonte [aqui](https://olhardigital.com.br/2023/03/24/pro/elon-musk-tentou-assumir-a-openai-falhou-e-descumpriu-promessa-de-investimento/), no início de 2018, Musk expressou preocupação de que a empresa estava ficando para trás de rivais como o Google na área de inteligência artificial, a solução oferecida por ele foi assumir o controle da OpenAi sozinho, a oferta foi rejeitada pelos outros fundadores e então Elon Musk saiu da empresa no mesmo ano, citando como justificativa conflito de interesses com o seu trabalho na Tesla.\n",
    "\n",
    "A empresa ficou sem grana até que apareceu a Microsoft aproveitando a oportunidade para financiar os projetos da OpenAI e garantir licenças para usar o IA desenvolvido em seus produtos (novo [Bing](https://www.bing.com/) e [Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)).\n",
    "\n",
    "Elon Musk não gostou muito disso e chegou a colocar no twitter que a OpenAi “se tornou uma empresa de código fechado” focada em “lucro máximo” e “efetivamente controlada pela Microsoft”.\n",
    "\n",
    "O espertinho pede pausa de apenas 6 meses no desenvolvimento da IA (leia a matéria [aqui](https://www.techtudo.com.br/noticias/2023/03/inteligencia-artificial-tudo-sobre-a-carta-aberta-assinada-por-elon-musk-edsoftwares.ghtml)). Mas segundo os boatos ele pode estar planejando o seu próprio ChatGPT para competir diretamente com o ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f915507",
   "metadata": {},
   "source": [
    "## 1.3 - Produtos <a class=\"anchor\" id=\"1_3\"></a>\n",
    "\n",
    "### 1.3.1 -  [DALL·E](https://openai.com/product/dall-e-2) <a class=\"anchor\" id=\"1_3_1\"></a>\n",
    "\n",
    "É um modelo de inteligência artificial desenvolvido pela OpenAI que gera imagens a partir de descrições textuais. \n",
    "\n",
    "O nome DALL·E é uma referência ao artista surrealista [Salvador Dali](https://pt.wikipedia.org/wiki/Salvador_Dal%C3%AD) e ao personagem de animação [WALL·E](https://pt.wikipedia.org/wiki/WALL-E).\n",
    "\n",
    "O DALL·E foi treinado em um grande conjunto de dados que inclui imagens e suas descrições correspondentes. A partir dessas informações, o modelo é capaz de gerar imagens realistas que correspondem às descrições dadas. O modelo é capaz de criar imagens de objetos, animais, paisagens e até mesmo cenas surreais que não existem na realidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fa2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d37928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-8OQ1cCRSSv0JBmMYFHcImXb7/user-wfObYe6EuRPlAi4K0Cuz48x3/img-IyOBWaINBOIbnZFuN4H6JVJA.png?st=2023-04-10T12%3A15%3A56Z&se=2023-04-10T14%3A15%3A56Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-10T00%3A13%3A49Z&ske=2023-04-11T00%3A13%3A49Z&sks=b&skv=2021-08-06&sig=UsHtPfIUsedh2CLjHUZJ2Q43jtdHzBhx59%2ByLjWsfXA%3D'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Image.create(\n",
    "  prompt=\"A pig flying with unicorn horns and playing a ukulele\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01540076",
   "metadata": {},
   "source": [
    "![](imgs/porco-tocando-ukulele.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a3687",
   "metadata": {},
   "source": [
    "![](imgs/preço-dale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815910b",
   "metadata": {},
   "source": [
    "### 1.3.2 - [Whisper](https://openai.com/research/whisper) <a class=\"anchor\" id=\"1_3_2\"></a>\n",
    "\n",
    "É um modelo de reconhecimento automático de fala treinado em 680.000 horas de dados supervisionados multilíngues e multitarefas coletados na web. \n",
    "\n",
    "Este modelo lida bem com sotaques, ruído de fundo e linguagem técnica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba14b898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi, eu gostaria de falar que tem um vassamento aqui na CETAI, como eu faço?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio_name = \"sotaque-cloves\"\n",
    "audio_name = \"colono-alemão\"\n",
    "# audio_name = \"baiano\"\n",
    "# audio_name = \"mineiro\"\n",
    "\n",
    "prompt = \"A transcrição são audio de pessoas que estão reportando um vazamento de água de uma \\\n",
    "adutora da empresa CEDAE.\"\n",
    "\n",
    "audio_file = open(f\"audios/{audio_name}.mp3\", \"rb\")\n",
    "\n",
    "transcript = openai.Audio.transcribe(model=\"whisper-1\", file=audio_file) #, prompt=prompt)\n",
    "transcript[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca8716",
   "metadata": {},
   "source": [
    "![](imgs/preço-whisper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d085e",
   "metadata": {},
   "source": [
    "### 1.3.3 - [Moderation](https://platform.openai.com/docs/guides/moderation/overview) <a class=\"anchor\" id=\"1_3_3\"></a>\n",
    "\n",
    "Os modelos fornecem capacidades de classificação que procuram conteúdo nas seguintes categorias: ódio, ódio/ameaças, auto-mutilação, sexual, sexual/menores, violência e violência/gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee9da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054a3f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7f967b87ac50> JSON: {\n",
       "  \"categories\": {\n",
       "    \"hate\": false,\n",
       "    \"hate/threatening\": false,\n",
       "    \"self-harm\": false,\n",
       "    \"sexual\": false,\n",
       "    \"sexual/minors\": false,\n",
       "    \"violence\": true,\n",
       "    \"violence/graphic\": false\n",
       "  },\n",
       "  \"category_scores\": {\n",
       "    \"hate\": 0.0006555579020641744,\n",
       "    \"hate/threatening\": 0.0001566615974297747,\n",
       "    \"self-harm\": 3.551872040929993e-08,\n",
       "    \"sexual\": 0.0005339246126823127,\n",
       "    \"sexual/minors\": 4.537509923352445e-08,\n",
       "    \"violence\": 0.9994120597839355,\n",
       "    \"violence/graphic\": 5.163837340660393e-05\n",
       "  },\n",
       "  \"flagged\": true\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Moderation.create(\n",
    "    input=\"i will kill you bitch\"\n",
    ")\n",
    "output = response[\"results\"][0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7c69e",
   "metadata": {},
   "source": [
    "### 1.3.4 - Modelos de chat complition <a class=\"anchor\" id=\"1_3_4\"></a>\n",
    "\n",
    "Estes modelos funcionam como uma espécie de autocomplete avançado. São eles:\n",
    "\n",
    "- [GPT-4](https://platform.openai.com/docs/models/gpt-4)\n",
    "- [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)\n",
    "- [GPT-3](https://platform.openai.com/docs/models/gpt-3): \n",
    "\n",
    "É possível:\n",
    "\n",
    "- Rascunhar um e-mail ou outro tipo de texto\n",
    "- Escrever código em Python\n",
    "- Responder perguntas sobre um conjunto de documentos\n",
    "- Criar agentes de conversação\n",
    "- Dar ao seu software uma interface em linguagem natural\n",
    "- Dar aulas particulares em uma variedade de disciplinas\n",
    "- Traduzir idiomas\n",
    "- Simular personagens para jogos de vídeo e muito mais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658bdc3",
   "metadata": {},
   "source": [
    "Para personalizar um modelo de chat passando um array de de objetos mensagens que seguem as seguintes regras:\n",
    "\n",
    "- system: um prompt que seta o comportamento do assistente\n",
    "- user: exemplos de mensagens de usuários que ajudam o assistente a se contextualizar melhor.\n",
    "- assistant: mensagens que o assistente deve retornar dada a pergunta do user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c9170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_messages(prompt_name: str) -> str:\n",
    "    with open(f\"prompts/{prompt_name}.json\") as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14b6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = read_messages(\"CEDAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68894a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(input):\n",
    "    if input:\n",
    "        messages.append({\"role\": \"user\", \"content\": input})\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            messages=messages, \n",
    "            temperature=0, \n",
    "            max_tokens=500\n",
    "        )\n",
    "        reply = chat.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14370cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claro, para registrar o vazamento, preciso de algumas informações. Qual é o seu endereço completo?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(\"quero reportar um vazamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c319bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, mas não consegui encontrar o endereço que você informou. Poderia verificar se o endereço está correto ou me fornecer um endereço válido para que eu possa ajudá-lo a registrar o vazamento?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(\"Rua Fictícia, n 42, Lapa, Rio de Janeiro - RJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a174b50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obrigada pelas informações. O vazamento está causando falta de água?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(\"R. Antônio Storino, 76 - Vila da Penha, Rio de Janeiro - RJ, 21221-460\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c067ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obrigada pela informação. O seu código de atendimento é 169832. Obrigada por reportar o vazamento. A equipe responsável será acionada para realizar a verificação e o reparo necessário.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot(\"Não\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c965bd",
   "metadata": {},
   "source": [
    "![](imgs/preço-chatgpt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4bfcf",
   "metadata": {},
   "source": [
    "### 1.3.5 - Embeddings e um pouco de como o chatGPT funciona <a class=\"anchor\" id=\"1_3_5\"></a>\n",
    "\n",
    "![](imgs/preço-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0d280",
   "metadata": {},
   "source": [
    "Livro [What Is ChatGPT Doing ... and Why Does It Work?](https://www.amazon.com.br/gp/product/B0BY59PT5Z/ref=ppx_yo_dt_b_d_asin_title_o00?ie=UTF8&psc=1) e pode ser lido gratuitamente [aqui](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaafe70",
   "metadata": {},
   "source": [
    "![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img101-edit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ddef2",
   "metadata": {},
   "source": [
    "- Uso mais baixo nível\n",
    "- Requer conhecimento especializado de ML\n",
    "- Uso para treino de modelos de ML como mercanismos de busca, clusterização, classificação, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050d6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vector(text: str):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b358d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_embedding_vector(\"quero pedir uma pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdfd40c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008244441822171211,\n",
       " -0.004389776382595301,\n",
       " -0.0020984436850994825,\n",
       " -0.015343994833528996,\n",
       " 0.007802663836628199,\n",
       " 0.005298220086842775,\n",
       " 0.0037115542218089104,\n",
       " 0.003842220874503255,\n",
       " 0.005649775732308626,\n",
       " -0.01518221665173769]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4866faa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb913b",
   "metadata": {},
   "source": [
    "Estes embeddings são usados pelo chatGPT para encontrar o próximo token de uma resposta.\n",
    "\n",
    "Também se usa a ideia dos [n-gramas](https://en.wikipedia.org/wiki/N-gram), ou seja, com base em n palavras dadas, buscar a melhor palavra que deve vir a seguir.\n",
    "\n",
    "Ex: \"cada macaco no seu...\"\n",
    "\n",
    "Geralmente se escolhe a palavra com maior probabilidade, mas o chatGPT também usa palavras de menor probabilidade, isso o deixa mais \"criativo\", com isso ele consegue gerar diferentes respostas para um mesmo prompt. \n",
    "\n",
    "Existe um parâmetro chamado de \"temperatura\" onde é possível regular com que frequência ele deve escolher palavras com menor probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdc85f",
   "metadata": {},
   "source": [
    "![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img104.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036654e",
   "metadata": {},
   "source": [
    "O chat representa uma sentença como um vetor/ponto numérico embedado num “espaço vetorial linguístico” de 12.288 dimensões. \n",
    "\n",
    "O pipeline que acontece dentro da \"mente\" do chatGPT é basicamente o seguinte:\n",
    "\n",
    "- quebra seu texto em [tokens](https://platform.openai.com/tokenizer)\n",
    "- a primeira camada da RNA faz o encoding de cada token para um vetor embedding de 12.288 dimensões\n",
    "- Pega a posição de cada token (valores interios de 0 a n) e gera outro vetor to tipo embedding\n",
    "- os vetores e cada tokens e o vetor que representa suas posições no texto são adicionados para produzir um único vetor (e uma das coisas feitas por tentativa e erro e que deram certo e não conseguimos porque isso funciona)\n",
    "- Este vetor final então passa por 96 “blocos de atenção”\n",
    "- Cada “bloco de atenção” possui 96 \"cabeças de atenção”\n",
    "- estas \"cabeças de atenção” “olham pra trás” e empacotam os tokens anteriores de uma forma que a ajude a encontrar o próximo token. Elas podem voltar atrás em busca de relações sintáticas entre estes tokens, por exemplo, verbos podem fazer referência a substantivos que se localizam muitas palavras atrás. A ideia é recombinar pedaços deste vetor do tipo embadding que estão associados.\n",
    "- Depois de passar por estas “cabeças e blocos de atenção” o vetor de 12.288 dimensões passa por um camada da rede neural “totalmente conectada”.\n",
    "- Isso se repete pelos próximos 95 blocos de atenção, cada um deles focando em características diferentes do texto.\n",
    "- O chatGPT então pega o último embedding da coleção de blocos de atenção e decodifica ele para produzir uma lista de probabilidades que irá definir qual token deve vir depois.\n",
    "\n",
    "No final deste processo, o chatGPT teve que fazer mais de 175 bilhões de cálculos, isso para cada token de uma resposta que ele compila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6733f2a",
   "metadata": {},
   "source": [
    "Prever o próximo token é como decidir para qual caminho tomar neste espaço. \n",
    "\n",
    "Haveria uma “lei de movimentação da semântica” que permitiria andar por este espaço mantendo o texto coerente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adb9c5",
   "metadata": {},
   "source": [
    "![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img107.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7559d2f",
   "metadata": {},
   "source": [
    "> And, yes, this seems like a mess—and doesn’t do anything to particularly encourage the idea that one can expect to identify “mathematical-physics-like” “semantic laws of motion” by empirically studying “what ChatGPT is doing inside”. But perhaps we’re just looking at the “wrong variables” (or wrong coordinate system) and if only we looked at the right one, we’d immediately see that ChatGPT is doing something “mathematical-physics-simple” like following geodesics. But as of now, we’re not ready to “empirically decode” from its “internal behavior” what ChatGPT has “discovered” about how human language is “put together”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6c2bc",
   "metadata": {},
   "source": [
    "O chatGPT nos mostrou que modelar a linguagem humana não é um tarefa tão complicada como se imaginava, um rede com “apenas” 175 bilhões (o ser humano possui em média 86 bilhões de neurônios conectados por mais de 1 trilhão de sinapsis) de pesos sinápticos foi suficiente para capturar a essência da fala humana. Isso indica que há “leis da fala humana” por aí que podem ser descobertas. Se essas leis forem descobertas, há o potencial de fazermos o que o chatGPT faz de forma mais direta, eficiente e transparente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8a7bb",
   "metadata": {},
   "source": [
    "É incrível como cada funçãozinha fazendo suas continhas faz emergir um bom trabalho de geração de texto de fala humana. Não há nenhuma teoria que explique porque isso funciona.\n",
    "\n",
    "Isso é uma descoberta científica! De alguma maneira, redes RNAs como a do chatGPT são capazes de capturar a essência do que o cérebro humano faz para gerar linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7269ea6",
   "metadata": {},
   "source": [
    "# 2.0 - Customização dos modelos <a class=\"anchor\" id=\"2_0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a4292",
   "metadata": {},
   "source": [
    "## 2.1 -  [Prompt design/engineering](https://platform.openai.com/docs/guides/completion/introduction) <a class=\"anchor\" id=\"2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcb719",
   "metadata": {},
   "source": [
    "A resposta completada real que você vê pode ser diferente porque a **API é não-determinística por padrão**. Isso significa que você pode obter uma resposta completada ligeiramente diferente toda vez que chamar a API, mesmo se o seu prompt permanecer o mesmo. Definir a **temperatura para 0 tornará as saídas principalmente determinísticas, mas uma pequena quantidade de variabilidade pode permanecer**.\n",
    "\n",
    "Essa interface simples de entrada e saída de texto significa que você pode \"programar\" o modelo fornecendo instruções ou apenas alguns exemplos do que deseja que ele faça. **O sucesso geralmente depende da complexidade da tarefa e da qualidade do seu prompt**. Uma boa regra geral é pensar em como você escreveria um problema matemático para um aluno do ensino fundamental resolver. Um prompt bem escrito fornece informações suficientes para que o modelo saiba o que você quer e como deve responder.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcebe9",
   "metadata": {},
   "source": [
    "Segue material para estudo de como criar prompts de qualidade: [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8f483",
   "metadata": {},
   "source": [
    "## 2.2 - [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning) <a class=\"anchor\" id=\"2_2\"></a>\n",
    "\n",
    "### 2.2.1 - Quando usar? <a class=\"anchor\" id=\"2_2_1\"></a>\n",
    "\n",
    "- Aplicações de domínio específico\n",
    "- Dados de treinamento limitados\n",
    "- Transferência de aprendizado\n",
    "\n",
    "### 2.2.2 - Consigo fazer o fine tuning do chatCPT 3.5 e 4? <a class=\"anchor\" id=\"2_2_2\"></a>\n",
    "\n",
    "Não, só está disponível para os seguintes modelos base da família GPT:\n",
    "\n",
    "- **ada**: capaz de tarefas muito simples, geralmente o modelo mais rápido da série GPT-3 e de menor custo.\n",
    "- **babbage**: capaz de tarefas diretas, muito rápidas e de baixo custo.\n",
    "- **curie**: muito capaz, mas mais rápido e com custo menor que Davinci.\n",
    "- **davinci**: Modelo GPT-3 mais capaz. Pode fazer qualquer tarefa que os outros modelos podem fazer, frequentemente com maior qualidade.\n",
    "\n",
    "![](imgs/preco-modelos-base.png)\n",
    "\n",
    "### 2.2.3 - Vantages <a class=\"anchor\" id=\"2_2_3\"></a>\n",
    "\n",
    "- Resultados melhores que os gerados através de prompts design\n",
    "- É possível gerar muito mais exemplos de treino do que se pode usando prompts\n",
    "- Economiza tokens devido aos prompts mais curtos\n",
    "- Requisições de menor latência \n",
    "\n",
    "### 2.2.4 - Como fazer o fine tuning? <a class=\"anchor\" id=\"2_2_4\"></a>\n",
    "\n",
    "Os passos para ter um modelo personalizado usando fine-tuning são:\n",
    "\n",
    "- Preparar e fazer upload dos dados de treino (há um CLI da openAI para subir e treinar estes dados)\n",
    "- Treinar o modelo\n",
    "- usar seu modelo personalizado\n",
    "\n",
    "A documentação da opneAI sugere ter algumas centenas de exemplos de treino, quanto mais, melhor.\n",
    "\n",
    "### 2.2.5 - Os dados de treino serão usados para alimentar o chatGPT e se tornarem públicos? <a class=\"anchor\" id=\"2_2_5\"></a>\n",
    "\n",
    "Os dados transitados via consumo de API não serão usados, apenas os que forem gerados pelo consumo de produtos gratuitos que estão disponíveis através de alguma UI da plataforma da openAI.\n",
    "\n",
    "Seguem as fontes dessa informação:\n",
    "\n",
    "- [Terms of use](https://openai.com/policies/terms-of-use)\n",
    "- [How your data is used to improve model performance](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7450bdc",
   "metadata": {},
   "source": [
    "# 3.0 - IA pode desenvolver conciência, desejos e experiências subjetivas? <a class=\"anchor\" id=\"3_0\"></a>\n",
    "\n",
    "\n",
    "Em última análise, toda rede neural corresponde apenas a alguma função matemática geral – embora possa ser confuso escrever. Segue um exemplo:\n",
    "\n",
    "![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img49.png)\n",
    "\n",
    "Eu tendo a acreditar que uma função que você poderia calcular na mão (se tivesse tempo) possa gerar consciência dentro de uma \"calculadora\" de silício que executa cálculos de base binária. \n",
    "\n",
    "O que o chatGPT fez foi capturar a \"física\", os padrões, a essência da fala humana com base nos dados textuais, com base nestes padrões encontradas pela sua RNA, ele consegue gerar um texto sem precisar de alguma conciência para isso.\n",
    "\n",
    "> \"Há vários caminhos diferentes que levam a uma grande inteligência e apenas alguns desses caminhos envolvem a tomada de conciência. Assim como aviões voam mais rápido que aves sem jamais desenvolver penas, também computadores podem resolver problemas muito melhor do que mamíferos sem jamais desenvolver sentimentos. Poderiam tornar-se superinteligentes mesmo tendo conciência zero.\" _ *Yuval Noah Harari em 21 lições para o século 21*\n",
    "\n",
    "[Documentário BBC: Inteligência artificial pode ter consciência?](https://www.youtube.com/watch?v=4SPwc1AyMsk&t=88s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
